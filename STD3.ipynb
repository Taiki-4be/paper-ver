{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "import argparse\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import dgl.nn as dglnn\n",
    "import tqdm\n",
    "from dgl.data.rdf import AIFBDataset, MUTAGDataset, BGSDataset, AMDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='RGCN')\n",
    "parser.add_argument(\"--dropout\", type=float, default=0,\n",
    "        help=\"dropout probability\")\n",
    "parser.add_argument(\"--n-hidden\", type=int, default=16,\n",
    "        help=\"number of hidden units\")\n",
    "parser.add_argument(\"--gpu\", type=int, default=-1,\n",
    "        help=\"gpu\")\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-2,\n",
    "        help=\"learning rate\")\n",
    "parser.add_argument(\"--n-bases\", type=int, default=-1,\n",
    "        help=\"number of filter weight matrices, default: -1 [use all]\")\n",
    "parser.add_argument(\"--n-layers\", type=int, default=2,\n",
    "        help=\"number of propagation rounds\")\n",
    "parser.add_argument(\"-e\", \"--n-epochs\", type=int, default=50,\n",
    "        help=\"number of training epochs\")\n",
    "parser.add_argument(\"--model_path\", type=str, default=None,\n",
    "        help='path for save the model')\n",
    "parser.add_argument(\"--l2norm\", type=float, default=0,\n",
    "        help=\"l2 norm coef\")\n",
    "parser.add_argument(\"--use-self-loop\", default=False, action='store_true',\n",
    "        help=\"include self feature as a special relation\")\n",
    "fp = parser.add_mutually_exclusive_group(required=False)\n",
    "fp.add_argument('--validation', dest='validation', action='store_true')\n",
    "fp.add_argument('--testing', dest='validation', action='store_false')\n",
    "parser.set_defaults(validation=True)\n",
    "\n",
    "args = parser.parse_args(args=['-e','100','--n-layers','5','--testing','--gpu','0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSVファイルからテンソルへの変換  \n",
    "daskというライブラリを経由して、テンソルへと変換  \n",
    "daskは大きなデータを扱う際にアシストしてくれるもの。学習の際に、サイズが大きいものをうまく扱う？  \n",
    "```\n",
    "import dask.dataframe as dd\n",
    "print(\"Loading CSV...\")\n",
    "test = dd.read_csv(\"position-pass-position.csv\", encoding = \"UTF-8\")\n",
    "\n",
    "print(\"Converting to Tensor...\")\n",
    "test_tensor = th.tensor(test)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### うまく行かなったのPandasで"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### グラフの構築\n",
    "作ったテンソルをDGLに読み込ませるだけ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel = pd.read_csv('data\\STD3\\data.csv')\n",
    "id = pd.read_csv('data\\STD3\\list.csv')\n",
    "pos_null = pd.read_csv('data\\STD3\\pn.csv')\n",
    "null_pos = pd.read_csv('data\\STD3\\_np.csv')\n",
    "t_link = pd.read_csv('data\\STD3\\link.csv')\n",
    "mini = pd.read_csv('data\\STD3\\mini\\list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphdata = {\n",
    "    ('position', 'pass','position'):(th.tensor(rel['fromid']),th.tensor(rel['toid'])),\n",
    "    ('position','temporal','position'):(th.tensor(t_link['fromid']),th.tensor(t_link['toid'])),\n",
    "    ('position','temporal','null'):(th.tensor(pos_null['fromid']),th.tensor(pos_null['nullid'])),\n",
    "    ('null','temporal','position'):(th.tensor(null_pos['nullid']),th.tensor(null_pos['toid']))\n",
    "}\n",
    "g = dgl.heterograph(graphdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label数２５\n",
    "labels = th.tensor(id['posid'])\n",
    "num_classes = 25 #len(g.ntypes)\n",
    "\"\"\"\n",
    "#label数４\n",
    "labels = th.tensor(id['newpos'])\n",
    "num_classes = 4 #len(g.ntypes)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "#label数８\n",
    "labels = th.tensor(id['8pos'])\n",
    "num_classes = 8 #len(g.ntypes)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'position'\n",
    "mask_wt = 0.7\n",
    "final_num = len(mini)\n",
    "num_node = g.num_nodes(category) - final_num\n",
    "\n",
    "mask = [1] * int(num_node * mask_wt) + [0] * (num_node - int(num_node * mask_wt))\n",
    "rd.shuffle(mask)\n",
    "mask = mask + [0] * final_num\n",
    "\n",
    "mask_wt = 0.2\n",
    "mask_tr = [1] * int(final_num * mask_wt) + [0] * (final_num - int(final_num * mask_wt))\n",
    "rd.shuffle(mask_tr)\n",
    "mask_tr = [0] * (num_node - final_num) + mask_tr\n",
    "\n",
    "train_mask = th.tensor(mask)\n",
    "test_mask = th.tensor(mask_tr)\n",
    "train_idx = th.nonzero(train_mask, as_tuple=False).squeeze()\n",
    "test_idx = th.nonzero(test_mask, as_tuple=False).squeeze()\n",
    "category_id = category\n",
    "for i, ntype in enumerate(g.ntypes):\n",
    "    if ntype == category:\n",
    "        category_id = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの定義\n",
    "今回はサンプルコードの中から`RelGraphConv`を引用する。使わないやつごと引用した"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelGraphConvLayer(nn.Module):\n",
    "    r\"\"\"Relational graph convolution layer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_feat : int\n",
    "        Input feature size.\n",
    "    out_feat : int\n",
    "        Output feature size.\n",
    "    rel_names : list[str]\n",
    "        Relation names.\n",
    "    num_bases : int, optional\n",
    "        Number of bases. If is none, use number of relations. Default: None.\n",
    "    weight : bool, optional\n",
    "        True if a linear layer is applied after message passing. Default: True\n",
    "    bias : bool, optional\n",
    "        True if bias is added. Default: True\n",
    "    activation : callable, optional\n",
    "        Activation function. Default: None\n",
    "    self_loop : bool, optional\n",
    "        True to include self loop message. Default: False\n",
    "    dropout : float, optional\n",
    "        Dropout rate. Default: 0.0\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_feat,\n",
    "                 out_feat,\n",
    "                 rel_names,\n",
    "                 num_bases,\n",
    "                 *,\n",
    "                 weight=True,\n",
    "                 bias=True,\n",
    "                 activation=None,\n",
    "                 self_loop=False,\n",
    "                 dropout=0.0):\n",
    "        super(RelGraphConvLayer, self).__init__()\n",
    "        self.in_feat = in_feat\n",
    "        self.out_feat = out_feat\n",
    "        self.rel_names = rel_names\n",
    "        self.num_bases = num_bases\n",
    "        self.bias = bias\n",
    "        self.activation = activation\n",
    "        self.self_loop = self_loop\n",
    "\n",
    "        self.conv = dglnn.HeteroGraphConv({\n",
    "                rel : dglnn.GraphConv(in_feat, out_feat, norm='right', weight=False, bias=False)\n",
    "                for rel in rel_names\n",
    "            })\n",
    "\n",
    "        self.use_weight = weight\n",
    "        self.use_basis = num_bases < len(self.rel_names) and weight\n",
    "        if self.use_weight:\n",
    "            if self.use_basis:\n",
    "                self.basis = dglnn.WeightBasis((in_feat, out_feat), num_bases, len(self.rel_names))\n",
    "            else:\n",
    "                self.weight = nn.Parameter(th.Tensor(len(self.rel_names), in_feat, out_feat))\n",
    "                nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "        # bias\n",
    "        if bias:\n",
    "            self.h_bias = nn.Parameter(th.Tensor(out_feat))\n",
    "            nn.init.zeros_(self.h_bias)\n",
    "\n",
    "        # weight for self loop\n",
    "        if self.self_loop:\n",
    "            self.loop_weight = nn.Parameter(th.Tensor(in_feat, out_feat))\n",
    "            nn.init.xavier_uniform_(self.loop_weight,\n",
    "                                    gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        \"\"\"Forward computation\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        g : DGLHeteroGraph\n",
    "            Input graph.\n",
    "        inputs : dict[str, torch.Tensor]\n",
    "            Node feature for each node type.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict[str, torch.Tensor]\n",
    "            New node features for each node type.\n",
    "        \"\"\"\n",
    "        g = g.local_var()\n",
    "        if self.use_weight:\n",
    "            weight = self.basis() if self.use_basis else self.weight\n",
    "            wdict = {self.rel_names[i] : {'weight' : w.squeeze(0)}\n",
    "                     for i, w in enumerate(th.split(weight, 1, dim=0))}\n",
    "        else:\n",
    "            wdict = {}\n",
    "\n",
    "        if g.is_block:\n",
    "            inputs_src = inputs\n",
    "            inputs_dst = {k: v[:g.number_of_dst_nodes(k)] for k, v in inputs.items()}\n",
    "        else:\n",
    "            inputs_src = inputs_dst = inputs\n",
    "\n",
    "        hs = self.conv(g, inputs, mod_kwargs=wdict)\n",
    "\n",
    "        def _apply(ntype, h):\n",
    "            if self.self_loop:\n",
    "                h = h + th.matmul(inputs_dst[ntype], self.loop_weight)\n",
    "            if self.bias:\n",
    "                h = h + self.h_bias\n",
    "            if self.activation:\n",
    "                h = self.activation(h)\n",
    "            return self.dropout(h)\n",
    "        return {ntype : _apply(ntype, h) for ntype, h in hs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelGraphConvLayerHeteroAPI(nn.Module):\n",
    "    r\"\"\"Relational graph convolution layer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_feat : int\n",
    "        Input feature size.\n",
    "    out_feat : int\n",
    "        Output feature size.\n",
    "    rel_names : list[str]\n",
    "        Relation names.\n",
    "    num_bases : int, optional\n",
    "        Number of bases. If is none, use number of relations. Default: None.\n",
    "    weight : bool, optional\n",
    "        True if a linear layer is applied after message passing. Default: True\n",
    "    bias : bool, optional\n",
    "        True if bias is added. Default: True\n",
    "    activation : callable, optional\n",
    "        Activation function. Default: None\n",
    "    self_loop : bool, optional\n",
    "        True to include self loop message. Default: False\n",
    "    dropout : float, optional\n",
    "        Dropout rate. Default: 0.0\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_feat,\n",
    "                 out_feat,\n",
    "                 rel_names,\n",
    "                 num_bases,\n",
    "                 *,\n",
    "                 weight=True,\n",
    "                 bias=True,\n",
    "                 activation=None,\n",
    "                 self_loop=False,\n",
    "                 dropout=0.0):\n",
    "        super(RelGraphConvLayerHeteroAPI, self).__init__()\n",
    "        self.in_feat = in_feat\n",
    "        self.out_feat = out_feat\n",
    "        self.rel_names = rel_names\n",
    "        self.num_bases = num_bases\n",
    "        self.bias = bias\n",
    "        self.activation = activation\n",
    "        self.self_loop = self_loop\n",
    "\n",
    "        self.use_weight = weight\n",
    "        self.use_basis = num_bases < len(self.rel_names) and weight\n",
    "        if self.use_weight:\n",
    "            if self.use_basis:\n",
    "                self.basis = dglnn.WeightBasis((in_feat, out_feat), num_bases, len(self.rel_names))\n",
    "            else:\n",
    "                self.weight = nn.Parameter(th.Tensor(len(self.rel_names), in_feat, out_feat))\n",
    "                nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "        # bias\n",
    "        if bias:\n",
    "            self.h_bias = nn.Parameter(th.Tensor(out_feat))\n",
    "            nn.init.zeros_(self.h_bias)\n",
    "\n",
    "        # weight for self loop\n",
    "        if self.self_loop:\n",
    "            self.loop_weight = nn.Parameter(th.Tensor(in_feat, out_feat))\n",
    "            nn.init.xavier_uniform_(self.loop_weight,\n",
    "                                    gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        \"\"\"Forward computation\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        g : DGLHeteroGraph\n",
    "            Input graph.\n",
    "        inputs : dict[str, torch.Tensor]\n",
    "            Node feature for each node type.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict[str, torch.Tensor]\n",
    "            New node features for each node type.\n",
    "        \"\"\"\n",
    "        g = g.local_var()\n",
    "        if self.use_weight:\n",
    "            weight = self.basis() if self.use_basis else self.weight\n",
    "            wdict = {self.rel_names[i] : {'weight' : w.squeeze(0)}\n",
    "                     for i, w in enumerate(th.split(weight, 1, dim=0))}\n",
    "        else:\n",
    "            wdict = {}\n",
    "\n",
    "        inputs_src = inputs_dst = inputs\n",
    "\n",
    "        for srctype,_,_ in g.canonical_etypes:\n",
    "            g.nodes[srctype].data['h'] = inputs[srctype]\n",
    "\n",
    "        if self.use_weight:\n",
    "            g.apply_edges(fn.copy_u('h', 'm'))\n",
    "            m = g.edata['m']\n",
    "            for rel in g.canonical_etypes:\n",
    "                _, etype, _ = rel\n",
    "                g.edges[rel].data['h*w_r'] =  th.matmul(m[rel], wdict[etype]['weight'])\n",
    "        else:\n",
    "            g.apply_edges(fn.copy_u('h', 'h*w_r'))\n",
    "\n",
    "        g.update_all(fn.copy_e('h*w_r', 'm'), fn.sum('m', 'h'))\n",
    "\n",
    "        def _apply(ntype):\n",
    "            h = g.nodes[ntype].data['h']\n",
    "            if self.self_loop:\n",
    "                h = h + th.matmul(inputs_dst[ntype], self.loop_weight)\n",
    "            if self.bias:\n",
    "                h = h + self.h_bias\n",
    "            if self.activation:\n",
    "                h = self.activation(h)\n",
    "            return self.dropout(h)\n",
    "        return {ntype : _apply(ntype) for ntype in g.dsttypes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelGraphEmbed(nn.Module):\n",
    "    r\"\"\"Embedding layer for featureless heterograph.\"\"\"\n",
    "    def __init__(self,\n",
    "                 g,\n",
    "                 embed_size,\n",
    "                 embed_name='embed',\n",
    "                 activation=None,\n",
    "                 dropout=0.0):\n",
    "        super(RelGraphEmbed, self).__init__()\n",
    "        self.g = g\n",
    "        self.embed_size = embed_size\n",
    "        self.embed_name = embed_name\n",
    "        self.activation = activation\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # create weight embeddings for each node for each relation\n",
    "        self.embeds = nn.ParameterDict()\n",
    "        for ntype in g.ntypes:\n",
    "            embed = nn.Parameter(th.Tensor(g.number_of_nodes(ntype), self.embed_size))\n",
    "            nn.init.xavier_uniform_(embed, gain=nn.init.calculate_gain('relu'))\n",
    "            self.embeds[ntype] = embed\n",
    "\n",
    "    def forward(self, block=None):\n",
    "        \"\"\"Forward computation\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        block : DGLHeteroGraph, optional\n",
    "            If not specified, directly return the full graph with embeddings stored in\n",
    "            :attr:`embed_name`. Otherwise, extract and store the embeddings to the block\n",
    "            graph and return.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        DGLHeteroGraph\n",
    "            The block graph fed with embeddings.\n",
    "        \"\"\"\n",
    "        return self.embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityClassify(nn.Module):\n",
    "    def __init__(self,\n",
    "                 g,\n",
    "                 h_dim, out_dim,\n",
    "                 num_bases,\n",
    "                 num_hidden_layers=1,\n",
    "                 dropout=0,\n",
    "                 use_self_loop=False):\n",
    "        super(EntityClassify, self).__init__()\n",
    "        self.g = g\n",
    "        self.h_dim = h_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.rel_names = list(set(g.etypes))\n",
    "        self.rel_names.sort()\n",
    "        if num_bases < 0 or num_bases > len(self.rel_names):\n",
    "            self.num_bases = len(self.rel_names)\n",
    "        else:\n",
    "            self.num_bases = num_bases\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.dropout = dropout\n",
    "        self.use_self_loop = use_self_loop\n",
    "\n",
    "        self.embed_layer = RelGraphEmbed(g, self.h_dim)\n",
    "        self.layers = nn.ModuleList()\n",
    "        # i2h\n",
    "        self.layers.append(RelGraphConvLayer(\n",
    "            self.h_dim, self.h_dim, self.rel_names,\n",
    "            self.num_bases, activation=F.relu, self_loop=self.use_self_loop,\n",
    "            dropout=self.dropout, weight=False))\n",
    "        # h2h\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            self.layers.append(RelGraphConvLayer(\n",
    "                self.h_dim, self.h_dim, self.rel_names,\n",
    "                self.num_bases, activation=F.relu, self_loop=self.use_self_loop,\n",
    "                dropout=self.dropout))\n",
    "        # h2o\n",
    "        self.layers.append(RelGraphConvLayer(\n",
    "            self.h_dim, self.out_dim, self.rel_names,\n",
    "            self.num_bases, activation=None,\n",
    "            self_loop=self.use_self_loop))\n",
    "\n",
    "    def forward(self, h=None, blocks=None):\n",
    "        if h is None:\n",
    "            # full graph training\n",
    "            h = self.embed_layer()\n",
    "        if blocks is None:\n",
    "            # full graph training\n",
    "            for layer in self.layers:\n",
    "                h = layer(self.g, h)\n",
    "        else:\n",
    "            # minibatch training\n",
    "            for layer, block in zip(self.layers, blocks):\n",
    "                h = layer(block, h)\n",
    "        return h\n",
    "\n",
    "    def inference(self, g, batch_size, device, num_workers, x=None):\n",
    "        \"\"\"Minibatch inference of final representation over all node types.\n",
    "\n",
    "        ***NOTE***\n",
    "        For node classification, the model is trained to predict on only one node type's\n",
    "        label.  Therefore, only that type's final representation is meaningful.\n",
    "        \"\"\"\n",
    "\n",
    "        if x is None:\n",
    "            x = self.embed_layer()\n",
    "\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            y = {\n",
    "                k: th.zeros(\n",
    "                    g.number_of_nodes(k),\n",
    "                    self.h_dim if l != len(self.layers) - 1 else self.out_dim)\n",
    "                for k in g.ntypes}\n",
    "\n",
    "            sampler = dgl.dataloading.MultiLayerFullNeighborSampler(1)\n",
    "            dataloader = dgl.dataloading.NodeDataLoader(\n",
    "                g,\n",
    "                {k: th.arange(g.number_of_nodes(k)) for k in g.ntypes},\n",
    "                sampler,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                drop_last=False,\n",
    "                num_workers=num_workers)\n",
    "\n",
    "            for input_nodes, output_nodes, blocks in tqdm.tqdm(dataloader):\n",
    "                block = blocks[0].to(device)\n",
    "\n",
    "                h = {k: x[k][input_nodes[k]].to(device) for k in input_nodes.keys()}\n",
    "                h = layer(block, h)\n",
    "\n",
    "                for k in h.keys():\n",
    "                    y[k][output_nodes[k]] = h[k].cpu()\n",
    "\n",
    "            x = y\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityClassify_HeteroAPI(nn.Module):\n",
    "    def __init__(self,\n",
    "                 g,\n",
    "                 h_dim, out_dim,\n",
    "                 num_bases,\n",
    "                 num_hidden_layers=1,\n",
    "                 dropout=0,\n",
    "                 use_self_loop=False):\n",
    "        super(EntityClassify_HeteroAPI, self).__init__()\n",
    "        self.g = g\n",
    "        self.h_dim = h_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.rel_names = list(set(g.etypes))\n",
    "        self.rel_names.sort()\n",
    "        if num_bases < 0 or num_bases > len(self.rel_names):\n",
    "            self.num_bases = len(self.rel_names)\n",
    "        else:\n",
    "            self.num_bases = num_bases\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.dropout = dropout\n",
    "        self.use_self_loop = use_self_loop\n",
    "\n",
    "        self.embed_layer = RelGraphEmbed(g, self.h_dim)\n",
    "        self.layers = nn.ModuleList()\n",
    "        # i2h\n",
    "        self.layers.append(RelGraphConvLayerHeteroAPI(\n",
    "            self.h_dim, self.h_dim, self.rel_names,\n",
    "            self.num_bases, activation=F.relu, self_loop=self.use_self_loop,\n",
    "            dropout=self.dropout, weight=False))\n",
    "        # h2h\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            self.layers.append(RelGraphConvLayerHeteroAPI(\n",
    "                self.h_dim, self.h_dim, self.rel_names,\n",
    "                self.num_bases, activation=F.relu, self_loop=self.use_self_loop,\n",
    "                dropout=self.dropout))\n",
    "        # h2o\n",
    "        self.layers.append(RelGraphConvLayerHeteroAPI(\n",
    "            self.h_dim, self.out_dim, self.rel_names,\n",
    "            self.num_bases, activation=None,\n",
    "            self_loop=self.use_self_loop))\n",
    "\n",
    "    def forward(self, h=None, blocks=None):\n",
    "        if h is None:\n",
    "            # full graph training\n",
    "            h = self.embed_layer()\n",
    "        if blocks is None:\n",
    "            # full graph training\n",
    "            for layer in self.layers:\n",
    "                h = layer(self.g, h)\n",
    "        else:\n",
    "            # minibatch training\n",
    "            for layer, block in zip(self.layers, blocks):\n",
    "                h = layer(block, h)\n",
    "        return h\n",
    "\n",
    "    def inference(self, g, batch_size, device, num_workers, x=None):\n",
    "        \"\"\"Minibatch inference of final representation over all node types.\n",
    "\n",
    "        ***NOTE***\n",
    "        For node classification, the model is trained to predict on only one node type's\n",
    "        label.  Therefore, only that type's final representation is meaningful.\n",
    "        \"\"\"\n",
    "\n",
    "        if x is None:\n",
    "            x = self.embed_layer()\n",
    "\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            y = {\n",
    "                k: th.zeros(\n",
    "                    g.number_of_nodes(k),\n",
    "                    self.h_dim if l != len(self.layers) - 1 else self.out_dim)\n",
    "                for k in g.ntypes}\n",
    "\n",
    "            sampler = dgl.dataloading.MultiLayerFullNeighborSampler(1)\n",
    "            dataloader = dgl.dataloading.NodeDataLoader(\n",
    "                g,\n",
    "                {k: th.arange(g.number_of_nodes(k)) for k in g.ntypes},\n",
    "                sampler,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                drop_last=False,\n",
    "                num_workers=num_workers)\n",
    "\n",
    "            for input_nodes, output_nodes, blocks in tqdm.tqdm(dataloader):\n",
    "                block = blocks[0].to(device)\n",
    "\n",
    "                h = {k: x[k][input_nodes[k]].to(device) for k in input_nodes.keys()}\n",
    "                h = layer(block, h)\n",
    "\n",
    "                for k in h.keys():\n",
    "                    y[k][output_nodes[k]] = h[k].cpu()\n",
    "\n",
    "            x = y\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ノード分類の実装\n",
    "ここから作ったグラフを使って学習を行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into train, validate, test\n",
    "if args.validation:\n",
    "    val_idx = train_idx[:len(train_idx) // 5]\n",
    "    train_idx = train_idx[len(train_idx) // 5:]\n",
    "else:\n",
    "    val_idx = train_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check cuda\n",
    "use_cuda = args.gpu >= 0 and th.cuda.is_available()\n",
    "if use_cuda:\n",
    "    th.cuda.set_device(args.gpu)\n",
    "    g = g.to('cuda:%d' % args.gpu)\n",
    "    labels = labels.cuda()\n",
    "    train_idx = train_idx.cuda()\n",
    "    test_idx = test_idx.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = EntityClassify(g,\n",
    "                       args.n_hidden,\n",
    "                       num_classes,\n",
    "                       num_bases=args.n_bases,\n",
    "                       num_hidden_layers=args.n_layers - 2,\n",
    "                       dropout=args.dropout,\n",
    "                       use_self_loop=args.use_self_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "        model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = th.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.l2norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "Epoch 00000 | Train Acc: 0.0824 | Train Loss: 2.0795 | Valid Acc: 0.0824 | Valid loss: 2.0795 | Time: nan\n",
      "Epoch 00001 | Train Acc: 0.1965 | Train Loss: 2.0735 | Valid Acc: 0.1965 | Valid loss: 2.0735 | Time: nan\n",
      "Epoch 00002 | Train Acc: 0.1975 | Train Loss: 2.0664 | Valid Acc: 0.1975 | Valid loss: 2.0664 | Time: nan\n",
      "Epoch 00003 | Train Acc: 0.1985 | Train Loss: 2.0576 | Valid Acc: 0.1985 | Valid loss: 2.0576 | Time: nan\n",
      "Epoch 00004 | Train Acc: 0.2029 | Train Loss: 2.0467 | Valid Acc: 0.2029 | Valid loss: 2.0467 | Time: nan\n",
      "Epoch 00005 | Train Acc: 0.2121 | Train Loss: 2.0336 | Valid Acc: 0.2121 | Valid loss: 2.0336 | Time: nan\n",
      "Epoch 00006 | Train Acc: 0.2138 | Train Loss: 2.0189 | Valid Acc: 0.2138 | Valid loss: 2.0189 | Time: 0.0310\n",
      "Epoch 00007 | Train Acc: 0.2122 | Train Loss: 2.0050 | Valid Acc: 0.2122 | Valid loss: 2.0050 | Time: 0.0305\n",
      "Epoch 00008 | Train Acc: 0.2105 | Train Loss: 1.9981 | Valid Acc: 0.2105 | Valid loss: 1.9981 | Time: 0.0300\n",
      "Epoch 00009 | Train Acc: 0.2089 | Train Loss: 1.9989 | Valid Acc: 0.2089 | Valid loss: 1.9989 | Time: 0.0302\n",
      "Epoch 00010 | Train Acc: 0.2104 | Train Loss: 1.9924 | Valid Acc: 0.2104 | Valid loss: 1.9924 | Time: 0.0304\n",
      "Epoch 00011 | Train Acc: 0.2101 | Train Loss: 1.9825 | Valid Acc: 0.2101 | Valid loss: 1.9825 | Time: 0.0301\n",
      "Epoch 00012 | Train Acc: 0.2119 | Train Loss: 1.9751 | Valid Acc: 0.2119 | Valid loss: 1.9751 | Time: 0.0300\n",
      "Epoch 00013 | Train Acc: 0.2142 | Train Loss: 1.9701 | Valid Acc: 0.2142 | Valid loss: 1.9701 | Time: 0.0301\n",
      "Epoch 00014 | Train Acc: 0.2156 | Train Loss: 1.9647 | Valid Acc: 0.2156 | Valid loss: 1.9647 | Time: 0.0301\n",
      "Epoch 00015 | Train Acc: 0.2158 | Train Loss: 1.9572 | Valid Acc: 0.2158 | Valid loss: 1.9572 | Time: 0.0301\n",
      "Epoch 00016 | Train Acc: 0.2161 | Train Loss: 1.9476 | Valid Acc: 0.2161 | Valid loss: 1.9476 | Time: 0.0301\n",
      "Epoch 00017 | Train Acc: 0.2156 | Train Loss: 1.9370 | Valid Acc: 0.2156 | Valid loss: 1.9370 | Time: 0.0301\n",
      "Epoch 00018 | Train Acc: 0.2156 | Train Loss: 1.9269 | Valid Acc: 0.2156 | Valid loss: 1.9269 | Time: 0.0302\n",
      "Epoch 00019 | Train Acc: 0.2159 | Train Loss: 1.9161 | Valid Acc: 0.2159 | Valid loss: 1.9161 | Time: 0.0305\n",
      "Epoch 00020 | Train Acc: 0.2178 | Train Loss: 1.9015 | Valid Acc: 0.2178 | Valid loss: 1.9015 | Time: 0.0305\n",
      "Epoch 00021 | Train Acc: 0.2238 | Train Loss: 1.8846 | Valid Acc: 0.2238 | Valid loss: 1.8846 | Time: 0.0307\n",
      "Epoch 00022 | Train Acc: 0.2383 | Train Loss: 1.8667 | Valid Acc: 0.2383 | Valid loss: 1.8667 | Time: 0.0306\n",
      "Epoch 00023 | Train Acc: 0.2579 | Train Loss: 1.8445 | Valid Acc: 0.2579 | Valid loss: 1.8445 | Time: 0.0306\n",
      "Epoch 00024 | Train Acc: 0.2779 | Train Loss: 1.8165 | Valid Acc: 0.2779 | Valid loss: 1.8165 | Time: 0.0306\n",
      "Epoch 00025 | Train Acc: 0.2941 | Train Loss: 1.7855 | Valid Acc: 0.2941 | Valid loss: 1.7855 | Time: 0.0306\n",
      "Epoch 00026 | Train Acc: 0.3142 | Train Loss: 1.7498 | Valid Acc: 0.3142 | Valid loss: 1.7498 | Time: 0.0306\n",
      "Epoch 00027 | Train Acc: 0.3337 | Train Loss: 1.7083 | Valid Acc: 0.3337 | Valid loss: 1.7083 | Time: 0.0306\n",
      "Epoch 00028 | Train Acc: 0.3580 | Train Loss: 1.6637 | Valid Acc: 0.3580 | Valid loss: 1.6637 | Time: 0.0306\n",
      "Epoch 00029 | Train Acc: 0.3792 | Train Loss: 1.6158 | Valid Acc: 0.3792 | Valid loss: 1.6158 | Time: 0.0306\n",
      "Epoch 00030 | Train Acc: 0.3946 | Train Loss: 1.5645 | Valid Acc: 0.3946 | Valid loss: 1.5645 | Time: 0.0306\n",
      "Epoch 00031 | Train Acc: 0.4094 | Train Loss: 1.5131 | Valid Acc: 0.4094 | Valid loss: 1.5131 | Time: 0.0306\n",
      "Epoch 00032 | Train Acc: 0.4286 | Train Loss: 1.4619 | Valid Acc: 0.4286 | Valid loss: 1.4619 | Time: 0.0305\n",
      "Epoch 00033 | Train Acc: 0.4495 | Train Loss: 1.4134 | Valid Acc: 0.4495 | Valid loss: 1.4134 | Time: 0.0305\n",
      "Epoch 00034 | Train Acc: 0.4681 | Train Loss: 1.3663 | Valid Acc: 0.4681 | Valid loss: 1.3663 | Time: 0.0305\n",
      "Epoch 00035 | Train Acc: 0.4881 | Train Loss: 1.3191 | Valid Acc: 0.4881 | Valid loss: 1.3191 | Time: 0.0304\n",
      "Epoch 00036 | Train Acc: 0.5008 | Train Loss: 1.2780 | Valid Acc: 0.5008 | Valid loss: 1.2780 | Time: 0.0304\n",
      "Epoch 00037 | Train Acc: 0.5089 | Train Loss: 1.2480 | Valid Acc: 0.5089 | Valid loss: 1.2480 | Time: 0.0304\n",
      "Epoch 00038 | Train Acc: 0.5336 | Train Loss: 1.1952 | Valid Acc: 0.5336 | Valid loss: 1.1952 | Time: 0.0305\n",
      "Epoch 00039 | Train Acc: 0.5502 | Train Loss: 1.1605 | Valid Acc: 0.5502 | Valid loss: 1.1605 | Time: 0.0305\n",
      "Epoch 00040 | Train Acc: 0.5660 | Train Loss: 1.1241 | Valid Acc: 0.5660 | Valid loss: 1.1241 | Time: 0.0305\n",
      "Epoch 00041 | Train Acc: 0.5884 | Train Loss: 1.0837 | Valid Acc: 0.5884 | Valid loss: 1.0837 | Time: 0.0305\n",
      "Epoch 00042 | Train Acc: 0.5978 | Train Loss: 1.0599 | Valid Acc: 0.5978 | Valid loss: 1.0599 | Time: 0.0305\n",
      "Epoch 00043 | Train Acc: 0.6133 | Train Loss: 1.0217 | Valid Acc: 0.6133 | Valid loss: 1.0217 | Time: 0.0304\n",
      "Epoch 00044 | Train Acc: 0.6271 | Train Loss: 0.9934 | Valid Acc: 0.6271 | Valid loss: 0.9934 | Time: 0.0304\n",
      "Epoch 00045 | Train Acc: 0.6361 | Train Loss: 0.9666 | Valid Acc: 0.6361 | Valid loss: 0.9666 | Time: 0.0304\n",
      "Epoch 00046 | Train Acc: 0.6499 | Train Loss: 0.9322 | Valid Acc: 0.6499 | Valid loss: 0.9322 | Time: 0.0305\n",
      "Epoch 00047 | Train Acc: 0.6654 | Train Loss: 0.9073 | Valid Acc: 0.6654 | Valid loss: 0.9073 | Time: 0.0304\n",
      "Epoch 00048 | Train Acc: 0.6758 | Train Loss: 0.8814 | Valid Acc: 0.6758 | Valid loss: 0.8814 | Time: 0.0304\n",
      "Epoch 00049 | Train Acc: 0.6925 | Train Loss: 0.8479 | Valid Acc: 0.6925 | Valid loss: 0.8479 | Time: 0.0304\n",
      "Epoch 00050 | Train Acc: 0.6982 | Train Loss: 0.8286 | Valid Acc: 0.6982 | Valid loss: 0.8286 | Time: 0.0304\n",
      "Epoch 00051 | Train Acc: 0.7100 | Train Loss: 0.8018 | Valid Acc: 0.7100 | Valid loss: 0.8018 | Time: 0.0304\n",
      "Epoch 00052 | Train Acc: 0.7223 | Train Loss: 0.7724 | Valid Acc: 0.7223 | Valid loss: 0.7724 | Time: 0.0303\n",
      "Epoch 00053 | Train Acc: 0.7317 | Train Loss: 0.7521 | Valid Acc: 0.7317 | Valid loss: 0.7521 | Time: 0.0303\n",
      "Epoch 00054 | Train Acc: 0.7404 | Train Loss: 0.7311 | Valid Acc: 0.7404 | Valid loss: 0.7311 | Time: 0.0303\n",
      "Epoch 00055 | Train Acc: 0.7508 | Train Loss: 0.7066 | Valid Acc: 0.7508 | Valid loss: 0.7066 | Time: 0.0303\n",
      "Epoch 00056 | Train Acc: 0.7575 | Train Loss: 0.6869 | Valid Acc: 0.7575 | Valid loss: 0.6869 | Time: 0.0303\n",
      "Epoch 00057 | Train Acc: 0.7693 | Train Loss: 0.6650 | Valid Acc: 0.7693 | Valid loss: 0.6650 | Time: 0.0303\n",
      "Epoch 00058 | Train Acc: 0.7806 | Train Loss: 0.6412 | Valid Acc: 0.7806 | Valid loss: 0.6412 | Time: 0.0303\n",
      "Epoch 00059 | Train Acc: 0.7866 | Train Loss: 0.6207 | Valid Acc: 0.7866 | Valid loss: 0.6207 | Time: 0.0303\n",
      "Epoch 00060 | Train Acc: 0.7927 | Train Loss: 0.6061 | Valid Acc: 0.7927 | Valid loss: 0.6061 | Time: 0.0303\n",
      "Epoch 00061 | Train Acc: 0.7967 | Train Loss: 0.5875 | Valid Acc: 0.7967 | Valid loss: 0.5875 | Time: 0.0303\n",
      "Epoch 00062 | Train Acc: 0.8050 | Train Loss: 0.5694 | Valid Acc: 0.8050 | Valid loss: 0.5694 | Time: 0.0303\n",
      "Epoch 00063 | Train Acc: 0.8086 | Train Loss: 0.5568 | Valid Acc: 0.8086 | Valid loss: 0.5568 | Time: 0.0303\n",
      "Epoch 00064 | Train Acc: 0.8121 | Train Loss: 0.5474 | Valid Acc: 0.8121 | Valid loss: 0.5474 | Time: 0.0303\n",
      "Epoch 00065 | Train Acc: 0.8209 | Train Loss: 0.5300 | Valid Acc: 0.8209 | Valid loss: 0.5300 | Time: 0.0302\n",
      "Epoch 00066 | Train Acc: 0.8293 | Train Loss: 0.5088 | Valid Acc: 0.8293 | Valid loss: 0.5088 | Time: 0.0302\n",
      "Epoch 00067 | Train Acc: 0.8309 | Train Loss: 0.4947 | Valid Acc: 0.8309 | Valid loss: 0.4947 | Time: 0.0302\n",
      "Epoch 00068 | Train Acc: 0.8382 | Train Loss: 0.4816 | Valid Acc: 0.8382 | Valid loss: 0.4816 | Time: 0.0302\n",
      "Epoch 00069 | Train Acc: 0.8438 | Train Loss: 0.4680 | Valid Acc: 0.8438 | Valid loss: 0.4680 | Time: 0.0301\n",
      "Epoch 00070 | Train Acc: 0.8428 | Train Loss: 0.4592 | Valid Acc: 0.8428 | Valid loss: 0.4592 | Time: 0.0301\n",
      "Epoch 00071 | Train Acc: 0.8520 | Train Loss: 0.4457 | Valid Acc: 0.8520 | Valid loss: 0.4457 | Time: 0.0301\n",
      "Epoch 00072 | Train Acc: 0.8545 | Train Loss: 0.4316 | Valid Acc: 0.8545 | Valid loss: 0.4316 | Time: 0.0301\n",
      "Epoch 00073 | Train Acc: 0.8546 | Train Loss: 0.4246 | Valid Acc: 0.8546 | Valid loss: 0.4246 | Time: 0.0301\n",
      "Epoch 00074 | Train Acc: 0.8602 | Train Loss: 0.4146 | Valid Acc: 0.8602 | Valid loss: 0.4146 | Time: 0.0301\n",
      "Epoch 00075 | Train Acc: 0.8634 | Train Loss: 0.4029 | Valid Acc: 0.8634 | Valid loss: 0.4029 | Time: 0.0301\n",
      "Epoch 00076 | Train Acc: 0.8656 | Train Loss: 0.3958 | Valid Acc: 0.8656 | Valid loss: 0.3958 | Time: 0.0301\n",
      "Epoch 00077 | Train Acc: 0.8680 | Train Loss: 0.3861 | Valid Acc: 0.8680 | Valid loss: 0.3861 | Time: 0.0301\n",
      "Epoch 00078 | Train Acc: 0.8715 | Train Loss: 0.3767 | Valid Acc: 0.8715 | Valid loss: 0.3767 | Time: 0.0301\n",
      "Epoch 00079 | Train Acc: 0.8724 | Train Loss: 0.3706 | Valid Acc: 0.8724 | Valid loss: 0.3706 | Time: 0.0301\n",
      "Epoch 00080 | Train Acc: 0.8755 | Train Loss: 0.3619 | Valid Acc: 0.8755 | Valid loss: 0.3619 | Time: 0.0301\n",
      "Epoch 00081 | Train Acc: 0.8776 | Train Loss: 0.3538 | Valid Acc: 0.8776 | Valid loss: 0.3538 | Time: 0.0301\n",
      "Epoch 00082 | Train Acc: 0.8791 | Train Loss: 0.3480 | Valid Acc: 0.8791 | Valid loss: 0.3480 | Time: 0.0302\n",
      "Epoch 00083 | Train Acc: 0.8824 | Train Loss: 0.3401 | Valid Acc: 0.8824 | Valid loss: 0.3401 | Time: 0.0304\n",
      "Epoch 00084 | Train Acc: 0.8855 | Train Loss: 0.3339 | Valid Acc: 0.8855 | Valid loss: 0.3339 | Time: 0.0304\n",
      "Epoch 00085 | Train Acc: 0.8876 | Train Loss: 0.3280 | Valid Acc: 0.8876 | Valid loss: 0.3280 | Time: 0.0304\n",
      "Epoch 00086 | Train Acc: 0.8907 | Train Loss: 0.3211 | Valid Acc: 0.8907 | Valid loss: 0.3211 | Time: 0.0304\n",
      "Epoch 00087 | Train Acc: 0.8931 | Train Loss: 0.3158 | Valid Acc: 0.8931 | Valid loss: 0.3158 | Time: 0.0304\n",
      "Epoch 00088 | Train Acc: 0.8949 | Train Loss: 0.3102 | Valid Acc: 0.8949 | Valid loss: 0.3102 | Time: 0.0316\n",
      "Epoch 00089 | Train Acc: 0.8968 | Train Loss: 0.3044 | Valid Acc: 0.8968 | Valid loss: 0.3044 | Time: 0.0315\n",
      "Epoch 00090 | Train Acc: 0.8984 | Train Loss: 0.2997 | Valid Acc: 0.8984 | Valid loss: 0.2997 | Time: 0.0315\n",
      "Epoch 00091 | Train Acc: 0.8992 | Train Loss: 0.2945 | Valid Acc: 0.8992 | Valid loss: 0.2945 | Time: 0.0315\n",
      "Epoch 00092 | Train Acc: 0.9013 | Train Loss: 0.2897 | Valid Acc: 0.9013 | Valid loss: 0.2897 | Time: 0.0315\n",
      "Epoch 00093 | Train Acc: 0.9032 | Train Loss: 0.2853 | Valid Acc: 0.9032 | Valid loss: 0.2853 | Time: 0.0315\n",
      "Epoch 00094 | Train Acc: 0.9043 | Train Loss: 0.2806 | Valid Acc: 0.9043 | Valid loss: 0.2806 | Time: 0.0314\n",
      "Epoch 00095 | Train Acc: 0.9052 | Train Loss: 0.2765 | Valid Acc: 0.9052 | Valid loss: 0.2765 | Time: 0.0314\n",
      "Epoch 00096 | Train Acc: 0.9059 | Train Loss: 0.2726 | Valid Acc: 0.9059 | Valid loss: 0.2726 | Time: 0.0314\n",
      "Epoch 00097 | Train Acc: 0.9088 | Train Loss: 0.2685 | Valid Acc: 0.9088 | Valid loss: 0.2685 | Time: 0.0314\n",
      "Epoch 00098 | Train Acc: 0.9099 | Train Loss: 0.2649 | Valid Acc: 0.9099 | Valid loss: 0.2649 | Time: 0.0313\n",
      "Epoch 00099 | Train Acc: 0.9110 | Train Loss: 0.2613 | Valid Acc: 0.9110 | Valid loss: 0.2613 | Time: 0.0313\n",
      "\n",
      "Test Acc: 0.8125 | Test loss: 3.6675\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "print(\"start training...\")\n",
    "dur = []\n",
    "model.train()\n",
    "for epoch in range(args.n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    if epoch > 5:\n",
    "        t0 = time.time()\n",
    "    logits = model()[category]\n",
    "    loss = F.cross_entropy(logits[train_idx], labels[train_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    t1 = time.time()\n",
    "\n",
    "    if epoch > 5:\n",
    "        dur.append(t1 - t0)\n",
    "    train_acc = th.sum(logits[train_idx].argmax(dim=1) == labels[train_idx]).item() / len(train_idx)\n",
    "    val_loss = F.cross_entropy(logits[val_idx], labels[val_idx])\n",
    "    val_acc = th.sum(logits[val_idx].argmax(dim=1) == labels[val_idx]).item() / len(val_idx)\n",
    "    print(\"Epoch {:05d} | Train Acc: {:.4f} | Train Loss: {:.4f} | Valid Acc: {:.4f} | Valid loss: {:.4f} | Time: {:.4f}\".\n",
    "         format(epoch, train_acc, loss.item(), val_acc, val_loss.item(), np.average(dur)))\n",
    "print()\n",
    "if args.model_path is not None:\n",
    "    th.save(model.state_dict(), args.model_path)\n",
    "\n",
    "model.eval()\n",
    "logits = model.forward()[category]\n",
    "test_loss = F.cross_entropy(logits[test_idx], labels[test_idx])\n",
    "test_acc = th.sum(logits[test_idx].argmax(dim=1) == labels[test_idx]).item() / len(test_idx)\n",
    "print(\"Test Acc: {:.4f} | Test loss: {:.4f}\".format(test_acc, test_loss.item()))\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "db27d65efe9566f42c956d5a32adbfc9fce65fe32866f8073254904fa4a1eb0e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('torch_rgcn_venv': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
