{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepGraphLibraly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. グラフ  \n",
    "グラフはentity（ノード）とrelation（エッジ）からなる。  \n",
    "DGLでは、グラフの構造、ノードとエッジを定義し、実行できるライブラリである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. 基本的な定義について   \n",
    "グラフは**entity**（ノード）と**relation**（エッジ）*を表すために用いられる構造である。  \n",
    "それぞれの定義は次のようになる\n",
    "> *__G=（V,E）__*  \n",
    "> **V**＝ノードの集合  \n",
    "> **E**＝エッジの集合   \n",
    "\n",
    "たとえばエッジ *( u , v )*∈* E*はノード *u , v* がつながっていることを示す。  \n",
    "また、グラフの**無向有向、同種異種、重みづけの有無**を定義することができる。  \n",
    "**異種グラフ**はノード、エッジのタイプが異なるもの  \n",
    "**マルチグラフ**は同じノード間に複数のエッジを許容するもの"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. グラフ、ノード、エッジ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ノードはそれぞれIDが割り当てられ、０から順に割り当てられる。  \n",
    "複数のノードを指定するには*Pytorch、Tensorflow*のテンソルを用いる。  \n",
    "**DGL**においてグラフを作成する際には `dgl.graph` を用いる  \n",
    "![example](https://data.dgl.ai/asset/image/user_guide_graphch_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch as th\n",
    "\n",
    "# edges 0->1, 0->2, 0->3, 1->3\n",
    "u, v = th.tensor([0, 0, 0, 1]), th.tensor([1, 2, 3, 3])\n",
    "g = dgl.graph((u, v))\n",
    "print(g) # number of nodes are inferred from the max node IDs in the given edges\n",
    "#Graph(num_nodes=4, num_edges=4,\n",
    "#      ndata_schemes={}\n",
    "#      edata_schemes={})\n",
    "\n",
    "# Node IDs\n",
    "print(g.nodes())\n",
    "#tensor([0, 1, 2, 3])\n",
    "# Edge end nodes\n",
    "print(g.edges())\n",
    "#(tensor([0, 0, 0, 1]), tensor([1, 2, 3, 3]))\n",
    "# Edge end nodes and edge IDs\n",
    "print(g.edges(form='all'))\n",
    "#(tensor([0, 0, 0, 1]), tensor([1, 2, 3, 3]), tensor([0, 1, 2, 3]))\n",
    "\n",
    "# If the node with the largest ID is isolated (meaning no edges),\n",
    "# then one needs to explicitly set the number of nodes\n",
    "g = dgl.graph((u, v), num_nodes=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仮に無向グラフを定義したい場合は `dgl.to_bidirected()` を用いる  \n",
    "ノードID、エッジIDには32bit,64bit整数が使えるが、**型を統一する必要がある**  \n",
    "デフォルトは64bitで変換するメゾットがある"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. ノード、エッジの機能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ノードとエッジにグラフ固有の特徴を格納するために、名前付きのプロパティを `ndata` 、 `edata` で定義できる。  \n",
    "サンプルでは*x , y , z*のプロパティを定義した"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch as th\n",
    "g = dgl.graph(([0, 0, 1, 5], [1, 2, 2, 0])) # 6 nodes, 4 edges\n",
    "g\n",
    "#Graph(num_nodes=6, num_edges=4,\n",
    "#      ndata_schemes={}\n",
    "#      edata_schemes={})\n",
    "g.ndata['x'] = th.ones(g.num_nodes(), 3)               # node feature of length 3\n",
    "g.edata['x'] = th.ones(g.num_edges(), dtype=th.int32)  # scalar integer feature\n",
    "g\n",
    "#Graph(num_nodes=6, num_edges=4,\n",
    "#      ndata_schemes={'x' : Scheme(shape=(3,), dtype=torch.float32)}\n",
    "#      edata_schemes={'x' : Scheme(shape=(,), dtype=torch.int32)})\n",
    "# different names can have different shapes\n",
    "g.ndata['y'] = th.randn(g.num_nodes(), 5)\n",
    "g.ndata['x'][1]                  # get node 1's feature\n",
    "#tensor([1., 1., 1.])\n",
    "g.edata['x'][th.tensor([0, 3])]  # get features of edge 0 and 3\n",
    "#tensor([1, 1], dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `edata`,`ndata` について\n",
    "* 整数、ベクトル、テンソルのみが使える  \n",
    "* ノード内のプロパティ名は重複できないが、ノード、エッジの間ではできる  \n",
    "* プロパティはテンソルによって順に割り当てられていき、その次元数はノードorエッジの数と同じでなければいけない  \n",
    "* 同一プロパティは同じ型でなければいけない  \n",
    "* プロパティのテンソルは上のコードの最後の４行のように格納されている  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 重みの定義\n",
    "グラフのエッジの重みは次のように定義できる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edges 0->1, 0->2, 0->3, 1->3\n",
    "edges = th.tensor([0, 0, 0, 1]), th.tensor([1, 2, 3, 3])\n",
    "weights = th.tensor([0.1, 0.6, 0.9, 0.7])  # weight of each edge\n",
    "g = dgl.graph(edges)\n",
    "g.edata['w'] = weights  # give it a name 'w'\n",
    "g\n",
    "#Graph(num_nodes=4, num_edges=4,\n",
    "#      ndata_schemes={}\n",
    "#      edata_schemes={'w' : Scheme(shape=(,), dtype=torch.float32)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. 外部ソースからグラフを作成\n",
    "外部ソースからのグラフの構築する方法は  \n",
    "* NetworkXやSciPyなどのライブラリからの変換  \n",
    "* ローカルディスクから読み込む  \n",
    "\n",
    "ほかにもいろいろ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NetworkX、Scipyから返還する方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch as th\n",
    "import scipy.sparse as sp\n",
    "spmat = sp.rand(100, 100, density=0.05) # 5% nonzero entries\n",
    "dgl.from_scipy(spmat)                   # from SciPy\n",
    "#Graph(num_nodes=100, num_edges=500,\n",
    "#      ndata_schemes={}\n",
    "#      edata_schemes={})\n",
    "\n",
    "import networkx as nx\n",
    "nx_g = nx.path_graph(5) # a chain 0-1-2-3-4\n",
    "dgl.from_networkx(nx_g) # from networkx\n",
    "#Graph(num_nodes=5, num_edges=8,\n",
    "#      ndata_schemes={}\n",
    "#      edata_schemes={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ディスクからのロード\n",
    "特定のフォーマットで記述されたcsvファイルから `tensor` や `ndarray` に変換する。  \n",
    "**pytorch**にはテンソルを保存 `torch,save` 、ロード `torch.load` するメゾットがある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 他にも\n",
    "`dgl.save_graphs()` `dgl.load_graphs()` によってグラフ自体を保存できる？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. 異種グラフについて\n",
    "ノード、エッジの種類が複数あり、関係が一般的な**トリプル**で示されるもの。無向グラフは、<span style=\"color:pink;\">エッジが一種類かつ双方向の異種グラフである</span>ことに注意。  \n",
    "ノード、エッジそれぞれIDで管理され、０から順に割り当てられる。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### グラフの作成\n",
    "異種グラフは、接続関係ごとに次のように定義される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch as th\n",
    "\n",
    "# Create a heterograph with 3 node types and 3 edges types.\n",
    "graph_data = {\n",
    "   ('drug', 'interacts', 'drug'): (th.tensor([0, 1]), th.tensor([1, 2])),\n",
    "   ('drug', 'interacts', 'gene'): (th.tensor([0, 1]), th.tensor([2, 3])),\n",
    "   ('drug', 'treats', 'disease'): (th.tensor([1]), th.tensor([2]))\n",
    "}\n",
    "g = dgl.heterograph(graph_data)\n",
    "g.ntypes\n",
    "#['disease', 'drug', 'gene']\n",
    "g.etypes\n",
    "#['interacts', 'interacts', 'treats']\n",
    "g.canonical_etypes\n",
    "#[('drug', 'interacts', 'drug'),\n",
    "# ('drug', 'interacts', 'gene'),\n",
    "# ('drug', 'treats', 'disease')]\n",
    "g\n",
    "#Graph(num_nodes={'disease': 3, 'drug': 3, 'gene': 4},\n",
    "#      num_edges={('drug', 'interacts', 'drug'): 2,\n",
    "#                 ('drug', 'interacts', 'gene'): 2,\n",
    "#                 ('drug', 'treats', 'disease'): 1},\n",
    "#      metagraph=[('drug', 'drug', 'interacts'),\n",
    "#                 ('drug', 'gene', 'interacts'),\n",
    "#                 ('drug', 'disease', 'treats')])\n",
    "g.metagraph().edges()\n",
    "#OutMultiEdgeDataView([('drug', 'drug'), ('drug', 'gene'), ('drug', 'disease')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 複数の型の扱い\n",
    "ノード、エッジのタイプが複数ある場合、メゾットを呼び出す際にタイプを指定できる。\n",
    "> `g.num_nodes()`　ノード数、タイプの指定がない場合、前ノード数  \n",
    "> `g.nodes()`　全ノードの出力。<span style=\"color:red;\">複数ある場合のみタイプの指定が必要</span>  \n",
    "> `g.nodes[''].data['']` or  `g.edges[''].data['']`　特定のプロパティの出力  \n",
    "> `g.ntypes` or `g.etypes`　タイプ名の出力  \n",
    "> `g.ndata[dgl.NTYPE]` or `g.edata[dgl.ETYPE]`　タイプIDの出力  \n",
    "> `g.ndata[dgl.NID]` or `g.edata[dgl.EID]`　ノード、エッジIDの出力  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### グラフの返還\n",
    "異種グラフから同種グラフに変換できる\n",
    "`hg = dgl.to_homopgeneous(g)` を使うと、トリプルだけコピーすることができ、特徴量などをコピーする場合、  \n",
    "`hg = dgl.to_homopgeneous(g, edata=['he'])`のようにする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. GPUの使用(<span style= \"color:red;\">重要</span>)\n",
    "GPU内で計算を行いたい場合、`g.to()`を用いて、GPUにコピーする必要がある  \n",
    "GPUにコピーされたグラフに関する計算はすべてGPUで行われ、結果もGPUに保存される。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch as th\n",
    "u, v = th.tensor([0, 1, 2]), th.tensor([2, 3, 4])\n",
    "g = dgl.graph((u, v))\n",
    "g.ndata['x'] = th.randn(5, 3)  # original feature is on CPU\n",
    "g.device\n",
    "#device(type='cpu')\n",
    "cuda_g = g.to('cuda:0')  # accepts any device objects from backend framework\n",
    "cuda_g.device\n",
    "#device(type='cuda', index=0)\n",
    "cuda_g.ndata['x'].device       # feature data is copied to GPU too\n",
    "#device(type='cuda', index=0)\n",
    "\n",
    "# A graph constructed from GPU tensors is also on GPU\n",
    "u, v = u.to('cuda:0'), v.to('cuda:0')\n",
    "g = dgl.graph((u, v))\n",
    "g.device\n",
    "#device(type='cuda', index=0)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8290a8480b737b4adfb273c8cea792d9f2fd9b0f5f0889cc2b72290fbe5b2970"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
